{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amazzoli/RL_introduction/blob/main/notebooks/Gridworld_with_Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZL615HVdM1Q"
      },
      "source": [
        "# Gridworld\n",
        "\n",
        "*Andrea Mazzolini*, andrea.mazzolini.90@gmail.com.\n",
        "\n",
        "\n",
        "Here we want to find the optimal strategy of a 2d grid-world problem using a model-free reinforcement-learning algorithm: **Q-learning**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD8PGgsAMQdZ"
      },
      "source": [
        "## Gridworld as a Markov Decision Process\n",
        "\n",
        "### States\n",
        "The state space is the physical cartesian space of the gridworld, identified by the two coordinates.\n",
        "Assuming that it is a square of side $d$, the whole state space is composed of $d^2$ states:\n",
        "\n",
        "$$\n",
        "\\mathcal{S} = \\{ 0, 1, \\ldots, d-1 \\} \\times \\{ 0, 1, \\ldots, d-1 \\}\n",
        "$$\n",
        "\n",
        "### Actions\n",
        "The actions of the agent are five: they can move to nearest neighbors or stay in the cell without moving, which can be expressed also as translation vectors.\n",
        "\n",
        "$$\n",
        "\\mathcal{A} = \\{ \\text{up}, \\text{left}, \\text{down}, \\text{right}, \\text{stay} \\} = \\{ (0,1), (-1,0), (0,-1), (1,0), (0,0)\\}\n",
        "$$\n",
        "\n",
        "Actually, these actions are not always possible in each state: the agent cannot cross boundaries or possible internal obstacles.\n",
        "This makes actions state dependent, for example if the agent is located on the left boundary they cannot go left, $\\mathcal{A}(0,y) = \\{ \\text{up}, \\text{down}, \\text{right}, \\text{stay} \\}$, or in a corner: $\\mathcal{A}(d-1,d-1) = \\{ \\text{left}, \\text{down}, \\text{stay} \\}$.\n",
        "\n",
        "### Transition probabilities\n",
        "\n",
        "The transition probabilities between states are deterministic: the next state is just the old state plus the translation action chosen by the agent:\n",
        "\n",
        "$$\n",
        "p(s_{t+1} | a_t, s_t) = \\delta (s_{t+1} - (a_t + s_t))\n",
        "$$\n",
        "\n",
        "### Rewards\n",
        "\n",
        "The rewards depend only on the arrival states, $r(s_{t+1})$, and are zero for all the states with the exception of some special cells that contain a treasure.\n",
        "Here we decide to not delete the treasure after the agent arrives on the cell, in a way that if they decide to stay they can collect the reward also at the future steps.\n",
        "\n",
        "### Aim of the game\n",
        "\n",
        "The agent has to learn to get the treasure by following the fastest path from a given initial cell.\n",
        "In formula, we want to maximize the return\n",
        "\n",
        "$$\n",
        "G_\\pi = \\sum_{t=0}^\\infty \\gamma^t\\, R_{t,\\pi} ,\n",
        "$$\n",
        "\n",
        "where $\\gamma$, the time horizon, introduces the time scale within which we want to acquire the treasure and $R$ is the stochastic variable corresponding to the reward acquired at time $t$ following the policy $\\pi$.\n",
        "The policy $\\pi(a | s)$ is our strategy that gives the probability of choosing $a \\in \\mathcal{A}(s)$ (the direction of the next step) from state $s$ (the coordinate in which we are), therefore is telling us how to navigate the map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p53uRswnMVGv"
      },
      "source": [
        "## Q-learning\n",
        "\n",
        "Q-learning is a reinforcement learning algorithm for any finite Markov Decision Processes (state and action space must be discrete and finite), which can converge to an optimal policy for maximizing an exponentially discounted return.\n",
        "It does not require a model (hence the connotation \"model-free\") of the environment and it just tries and learns from experience.\n",
        "For a \"model\" we mean the knowledge of the transition probabilities and reward function of the MDP. Differently, Value Iteration relies on this information.\n",
        "\n",
        "### Table for the state-action qualities\n",
        "\n",
        "The general idea is to build a table of estimates of \"goodness\" of each state and action pairs.\n",
        "This goodness of state $s$ and action $a$ is called quality function $Q^*(s,a)$, and it is the best possible return the agent can get from $s$ choosing $a$.\n",
        "\n",
        "$$\n",
        "Q^*(s,a) = \\max_{\\pi} \\; \\mathbb{E}_\\pi\\left[ G_\\pi \\; \\Big| \\; s_0 = s, \\; a_0 = a \\right]\n",
        "$$\n",
        "\n",
        "We then call ${Q}(s, a)$ (without star) quality table or quality matrix, our estimate for that function.\n",
        "It can be proven that, when the algorithm converges: $ Q(s,a) \\rightarrow Q^*(s,a)$.\n",
        "\n",
        "By assuming that $Q$ is a good estimate of $Q^*$, the best policy is deterministic and consists in choosing the action that leads to the best possible return:\n",
        "\n",
        "$$\n",
        "\\pi^*(a | s) = \\delta (a - \\text{argmax}_{b \\in \\mathcal{A}} Q(s,b))\n",
        "$$\n",
        "\n",
        "### Finding the quality matrix\n",
        "\n",
        "The core of the algorithm is a simple online quality update that in the long run will make our quality table converge to the best quality function.\n",
        "\n",
        "At time $t$ the learning agent is in the state $s_t$ and takes the action $a_t$ (later we specify how to choose the action). As a consequence it moves to a new state $S_{t+1}$ taking the reward $R_t$. Note that $S_{t+1}$ and $R_t$ are stochastic outcomes of the MDP (this is why they are written in capital letters), depending on the transition probabilities and the policy.\n",
        "The Q-learning update rule for the Quality is:\n",
        "\n",
        "$$\n",
        "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left(R_t + \\gamma \\max_{b \\in \\mathcal{A}} Q(S_{t+1}, b) - Q(s_t, a_t)\\right)\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is the discount factor and $\\alpha$ is a learning rate.\n",
        "\n",
        "Note that there is a reason for the update rule above to work.\n",
        "It is leading to a stable fixed point that is given by setting to zero the quantity in the parenthesis, which corresponds to\n",
        "\n",
        "$$\n",
        "Q(s_t, a_t) = R_t + \\gamma \\max_{b \\in \\mathcal{A}} Q(S_{t+1}, b) .\n",
        "$$\n",
        "\n",
        "This equation, that the update rule wants to satisfy, is a close relative of  the Bellman equation, $Q(s_t, a_t)^* = \\mathbb{E} \\left[ r_t + \\gamma \\max_b Q^*(s_{t+1}, b) \\right] $, which is satisfied for the optimal policy.\n",
        "There is however a small difference between the two: the Bellman equation has an average value over all the possible trajectories of our process, which can lead to different rewards and new states.\n",
        "The update rule considers only the single sample that we are seeing at that instance of the process.\n",
        "However, one can imagine that if we pass several times from the same state and action, in the limit of small learning rate, the update rule will average over all of them.\n",
        "In other words, the update rule for Q-learning implements the Bellman equation that will lead to the right quality function.\n",
        "\n",
        "\n",
        "### Pseudocode for Q-learning\n",
        "\n",
        "The algorithm consists in starting from an initial configuration for the quality table (e.g. all the entries for $s$ and $a$ have the same value), and then \"play the game\":\n",
        "$$\n",
        "s_0, a_0 \\rightarrow r_0, s_1, a_1 \\rightarrow r_1, s_2, a_2 \\rightarrow \\ldots\n",
        "$$\n",
        "The quality table will be updated at every step with the rule above.\n",
        "\n",
        "We still have to specify how to choose actions. Here we consider a simple and widely used protocol called *epsilon-greedy*. Let us define two way of choosing the action:\n",
        "- **Exploration** move, where the action is chosen uniformly at random among the possible actions from the state in which the agent is.\n",
        "- **Exploitation** move, where the action is taken as the one that maximizes my current qualities, which is the best action that I can take according to my estimates of the returns, $a_t = \\text{argmax}_b Q(s_t, b)$.\n",
        "\n",
        "An epsilon-greedy strategy says that the exploration move is chosen with probability $\\epsilon$, and the exploitation move with $1 - Ïµ$.\n",
        "\n",
        "Putting everything together, the core series of instructions for an epsilon-greedy Q-learning algorithm is the following:\n",
        "\n",
        " - Initialize the Q-matrix and choose the algorithm parameters $\\gamma$, $\\alpha$, $\\epsilon$.\n",
        " - Set the agent in the starting state $s_0$.\n",
        " - For $t = 1, \\ldots$ until convergence:\n",
        "> - With probability $\\epsilon$ choose $a_t$ at random from the possible actions, otherwise choose the action that maximizes the qualities $a_t = \\text{argmax}_b Q(s_t, b)$.\n",
        "> - Play a step in the game and get the new state and the reward $s_t, a_t \\rightarrow S_{t+1}, R_t$\n",
        "> - Update the quality matrix\n",
        "> $$\n",
        "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left(R_t + \\gamma \\max_b Q(S_{t+1}, b) - Q(s_t, a_t)\\right)\n",
        "$$\n",
        "\n",
        "### Episodic game and exploration scheduling\n",
        "\n",
        "Actually one usually introduces two tricks to speed up the covergence of the quality table.\n",
        "\n",
        "The first is to restart the game after a given number of steps (the state is forced to be $s_0$ again). Each of these runs is called episode. This is natural if there are terminal states and, at some point, the game finishes, but this is not our case. For us, this comes in handy bacause we can imagine that we are interested in a particular initial condition (the initial cell in which we put our agent). In this way, I force the algorithm to explore more around there and have better estimates of the quality.\n",
        "\n",
        "A second trick is to schedule the exporation parameter $\\epsilon$. Usually, I want the exploration to be large at the beginning, to have an approximate idea of all the possible qualities. Later, when I'm more confident about their values I want instead to focus on the best moves to have more fine-tuned estimates of them, forgetting about the bad actions.\n",
        "This transition from explortation to exploitation is typical of reinforcement learning. Quantitative and precise prescriptions on how to do it in practice typically are available only for extremely simple problems, and choosing the right protocol in general is more of an art than a science.\n",
        "\n",
        "Rewriting the pseudocode above we have:\n",
        "\n",
        " - Initialize the Q-matrix and choose the algorithm parameters $\\gamma$, $\\alpha$, $T_{episode}$ and how to schedule the exploration probability $Ïµ_e$ ($e$ is the episode index).\n",
        "> For episodes $e = 1, \\ldots$ until convergence:\n",
        "> - Set the agent in the starting state $s_0$.\n",
        "> - For steps in the episode $t = 1, \\ldots, T_{episode}$:\n",
        ">> - With probability $\\epsilon_e$ choose $a_t$ at random from the possible actions, otherwise choose the action that maximizes the Qualities $a_t = \\text{argmax}_b Q(s_t, b)$.\n",
        ">> - Play a step in the game and get the new state and the reward $s_t, a_t \\rightarrow S_{t+1}, R_t$\n",
        ">> - Update the quality matrix\n",
        ">> $$\n",
        "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left(R_t + \\gamma \\max_b Q(S_{t+1}, b) - Q(s_t, a_t)\\right)\n",
        "$$\n",
        "> - Decrease the exploration rate $\\epsilon_e$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Xp8OIAhgVB"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "### Environment class: the gridworld\n",
        "\n",
        "The Gridworld class contains all the information about the environment:\n",
        "- The info about the state space, the current state of the game and the initial state.\n",
        "- The set of possible actions.\n",
        "- The reward table: which reward the agent take in each cell (0 if none).\n",
        "\n",
        "The methods are:\n",
        "- `reset()`: the game is initialized at the beginning of a episode. Here the only initialization is to put the agent in the starting cell.\n",
        "- `step(action)`: update the agent state according to the `action` passed and compute reward. Returns the state after the transition (the movement) and the reward.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsmmSoZhhiqW",
        "cellView": "code"
      },
      "source": [
        "import numpy as np\n",
        "from copy import copy\n",
        "import sys\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colormaps as cm\n",
        "\n",
        "\n",
        "class Gridworld:\n",
        "\n",
        "  def __init__(self, grid_size, init_cell, rewards, obstacles=[]):\n",
        "    \"\"\"\n",
        "    Training environment for reinforcement learning: gridworld.\n",
        "    Args:\n",
        "    - grid_size, (int, int): defining the size of the 2d lattice\n",
        "    - init_cell, (int, int): coordinates from 0 to size-1 from which the agent starts to play\n",
        "    - rewards, list((int, int), float): list of the coordinates and values of the rewards\n",
        "    - obstacles, lits((int, int)): list of the coordinates of the obstacles\n",
        "    \"\"\"\n",
        "\n",
        "    # Define state space\n",
        "    self.state = None  # current state of the game\n",
        "    self.state_dim = grid_size\n",
        "    self.init_state = init_cell\n",
        "    self.obstacles = obstacles\n",
        "    # Cells that are not obstacles\n",
        "    self.states = [(i,j) for i in range(self.state_dim[0]) for j in range(self.state_dim[1]) if (i,j) not in self.obstacles]\n",
        "\n",
        "    # Define action space\n",
        "    self.action_dim = (5,)  # up, right, down, left, stay\n",
        "    self.action_dict = {\"up\": 0, \"right\": 1, \"down\": 2, \"left\": 3, \"stay\": 4}\n",
        "    self.action_coords = [(0, 1), (1, 0), (0, -1), (-1, 0), (0, 0)]  # translations\n",
        "    self.actions_allowed = self._build_allowed_actions(obstacles)\n",
        "\n",
        "    # Define rewards table\n",
        "    self.R = self._build_rewards(rewards)\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset agent state to its initial cell\"\"\"\n",
        "    self.state = self.init_state\n",
        "    return self.state\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Update agent state\"\"\"\n",
        "    state_next = (self.state[0] + self.action_coords[action][0],\n",
        "                  self.state[1] + self.action_coords[action][1])\n",
        "    # Collect reward\n",
        "    reward = self.R[self.state]\n",
        "    # Update state\n",
        "    self.state = state_next\n",
        "    return state_next, reward\n",
        "\n",
        "\n",
        "  def _build_allowed_actions(self, obstacles):\n",
        "    actions_allowed = dict()\n",
        "    Nx, Ny = self.state_dim\n",
        "    for x in range(Nx):\n",
        "      for y in range(Ny):\n",
        "        # Actions not allowed at the boundaries\n",
        "        actions_allowed[(x,y)] = [self.action_dict[\"stay\"]] # The stay action is always allowed\n",
        "        if (y > 0):\n",
        "          actions_allowed[(x,y)].append(self.action_dict[\"down\"])\n",
        "        if (y < Ny - 1):\n",
        "          actions_allowed[(x,y)].append(self.action_dict[\"up\"])\n",
        "        if (x > 0):\n",
        "          actions_allowed[(x,y)].append(self.action_dict[\"left\"])\n",
        "        if (x < Nx - 1):\n",
        "          actions_allowed[(x,y)].append(self.action_dict[\"right\"])\n",
        "        actions_allowed[(x,y)] = np.array(actions_allowed[(x,y)], dtype=int)\n",
        "        # Actions not allowed because of obstacles\n",
        "        for o in obstacles:\n",
        "          if (x+1,y) == o:\n",
        "            actions_allowed[(x,y)] = actions_allowed[(x,y)][actions_allowed[(x,y)] != self.action_dict[\"right\"]]\n",
        "          if (x-1,y) == o:\n",
        "            actions_allowed[(x,y)] = actions_allowed[(x,y)][actions_allowed[(x,y)] != self.action_dict[\"left\"]]\n",
        "          if (x,y+1) == o:\n",
        "            actions_allowed[(x,y)] = actions_allowed[(x,y)][actions_allowed[(x,y)] != self.action_dict[\"up\"]]\n",
        "          if (x,y-1) == o:\n",
        "            actions_allowed[(x,y)] = actions_allowed[(x,y)][actions_allowed[(x,y)] != self.action_dict[\"down\"]]\n",
        "    return actions_allowed\n",
        "\n",
        "\n",
        "  def _build_rewards(self, rewards):\n",
        "      R = np.zeros(self.state_dim, dtype=float)\n",
        "      for rew in rewards:\n",
        "        R[rew[0]] = rew[1]\n",
        "      return R\n",
        "\n",
        "  def display(self, ax, values=np.array([]), cmap=cm[\"Reds\"]):\n",
        "    \"\"\"\n",
        "    Plot the map on the axis object ax. It's possible to add a color to each cell, e.g. its value,\n",
        "    by passing a properly sized vectors of values\n",
        "    \"\"\"\n",
        "\n",
        "    cmap.set_bad(\"black\")\n",
        "    obstacle_mask = np.zeros(self.state_dim, dtype=bool)\n",
        "    for obs in obstacles:\n",
        "      obstacle_mask[obs[0], obs[1]] = True\n",
        "\n",
        "    if len(values)==0:\n",
        "      ax = sns.heatmap(obstacle_mask.T, cmap=cm[\"Greys\"], cbar=False,\n",
        "                       linewidths=0.1, linecolor='#222222')\n",
        "    else:\n",
        "      ax = sns.heatmap(values.T, mask=obstacle_mask.T, cmap=cmap,\n",
        "                       linewidths=0.1, linecolor='#222222', vmin=np.min(values[values != 0]))\n",
        "      ax.collections[0].colorbar.set_label(\"Value\", fontsize=14)\n",
        "\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xlabel('x', fontsize=14)\n",
        "    ax.set_ylabel('y', fontsize=14)\n",
        "    ax.scatter([start_cell[0]+0.5],[start_cell[1]+0.5], s=100, c='grey', label='Start')\n",
        "    for rew in rewards:\n",
        "      ax.scatter([rew[0][0]+0.5],[rew[0][1]+0.5], s=200*rew[1], c='#ffcc00', label='Reward:{}'.format(rew[1]), marker='*')\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "  def display_best_path(self, ax, Q, start_coord, lcolor='black', values=np.array([]), cmap=cm[\"Reds\"]):\n",
        "    \"\"\"\n",
        "    Plot the map and display the best path on the axis object ax.\n",
        "    \"\"\"\n",
        "\n",
        "    ax = self.display(ax, values, cmap)\n",
        "\n",
        "    s, count, best_action = start_coord, 0, 0\n",
        "    while best_action != 4 or count < self.state_dim[0]*self.state_dim[1]:\n",
        "      count += 1\n",
        "      best_action = np.argmax(Q[s[0], s[1], :])\n",
        "      new_s = s[0] + self.action_coords[best_action][0], s[1] + self.action_coords[best_action][1]\n",
        "      ax.plot([s[0]+0.5, new_s[0]+0.5], [s[1]+0.5, new_s[1]+0.5], c='black', lw=3)\n",
        "      s = new_s\n",
        "\n",
        "    return ax"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpXsKTUAMpbw"
      },
      "source": [
        "### Building the gridworld\n",
        "\n",
        "As stated before, we build a griword with two rewards: one large and far away from the starting state, one smaller and closer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzkBQx_mMn_-",
        "outputId": "63f7459a-be48-4b64-f42c-8e8e338acd60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "world_size = (10,12) # dimension of the gridworld\n",
        "\n",
        "start_cell = (2,3)\n",
        "obstacles = [(2,5), (3,5), (4,5), (5,5), (5,4), (5,3), (5,2), (5,1), (5,0),\n",
        "             (7,4), (7,5), (7,6), (7,7)]\n",
        "#rewards = [((8,6), 2), ((5, 10), 1)]\n",
        "rewards = [((5, 10), 1)]\n",
        "\n",
        "gridworld = Gridworld(world_size, start_cell, rewards, obstacles) # Building the world\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(world_size[0]/2.5, world_size[1]/2.5))\n",
        "ax = gridworld.display(ax) # And showing it\n",
        "\n",
        "plt.tight_layout()\n",
        "#plt.savefig('Gridwrold.png', dpi=200)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWmKisyJIfKG"
      },
      "source": [
        "## Agent class: the Q-learning algorithm\n",
        "\n",
        "\n",
        "This class defines how the agent chooses the action from each state and how to improve its strategy while playing.\n",
        "It employs a Q-learning algorithm with an epsilon-greedy policy.\n",
        "In particular:\n",
        "- `get_action(state)` returns an action using the epsilon greedy rule from a state.\n",
        "- `train(transition)` performs one step of the Q-learning update according to a transition of the game that consists in state, action, next state nad reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OhtaVbMiLWu"
      },
      "source": [
        "import operator\n",
        "\n",
        "class QL_agent:\n",
        "\n",
        "  def __init__(self, env, gamma, learning_rate=0.1, eps_decay=0.995, init_Q=0):\n",
        "\n",
        "    # Store the environment\n",
        "    self.env = env\n",
        "\n",
        "    # Agent learning parameters\n",
        "    self.epsilon = 1.0  # initial exploration probability\n",
        "    self.epsilon_decay = eps_decay  # epsilon decay after each episode\n",
        "    self.alpha = learning_rate\n",
        "    self.gamma = gamma  # reward discount factor\n",
        "\n",
        "    # Initialize Quality matrix\n",
        "    self.Q = np.ones(env.state_dim + env.action_dim, dtype=float)*init_Q\n",
        "\n",
        "\n",
        "  def get_action(self, state):\n",
        "    \"\"\"\n",
        "    Choose an action using an epsilon greedy policy: random with probability\n",
        "    epsilon, greedy otherwise.\n",
        "    \"\"\"\n",
        "    actions_allowed = self.env.actions_allowed[state]\n",
        "    if np.random.rand() < self.epsilon:  # explore\n",
        "      return np.random.choice(actions_allowed)\n",
        "    else:  # exploit\n",
        "      Q_s = self.Q[state[0], state[1], actions_allowed]\n",
        "      actions_greedy = actions_allowed[np.flatnonzero(Q_s == np.max(Q_s))]\n",
        "      return np.random.choice(actions_greedy)\n",
        "\n",
        "\n",
        "  def train(self, transition):\n",
        "    \"\"\"\n",
        "    Q-learning update\n",
        "    \"\"\"\n",
        "    (state, action, state_next, reward) = transition\n",
        "    sa = state + (action,)\n",
        "    td_error = reward + gamma * np.max(self.Q[state_next]) - self.Q[sa]\n",
        "    self.Q[sa] += self.alpha * td_error\n",
        "\n",
        "  @property\n",
        "  def values(self):\n",
        "    vals = np.zeros(self.env.state_dim)\n",
        "    for i in range(len(self.Q)):\n",
        "      for j in range(len(self.Q[0])):\n",
        "        vals[i,j] = np.max(self.Q[i,j])\n",
        "    return vals\n",
        "\n",
        "  def display_greedy_policy(self):\n",
        "    greedy_policy = np.zeros((self.env.state_dim[0], self.env.state_dim[1]), dtype=str)\n",
        "    for x in range(self.env.state_dim[0]):\n",
        "        for y in range(self.env.state_dim[1]):\n",
        "            greedy_policy[x, y] = str(np.argmax(self.Q[x, y, :]))\n",
        "    for o in obstacles:\n",
        "      greedy_policy[o[0],o[1]] = 'x'\n",
        "    print(\"\\nGreedy policy for gamma={}:\".format(self.gamma))\n",
        "    print(greedy_policy.T[::-1])\n",
        "    for (key, val) in sorted(self.env.action_dict.items(), key=operator.itemgetter(1)):\n",
        "      print(\" action['{}'] = {}\".format(key, val))\n",
        "    print(\"x: obstacle\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTLYP-ZZMCvB"
      },
      "source": [
        "### Main learning cycle\n",
        "\n",
        "It follows the pseudocode written before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3KcsznilZl",
        "outputId": "3ee174e7-7343-4ddf-b4ab-520366f89fc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "n_episodes = 1000\n",
        "n_steps_in_episode = 100\n",
        "eps_decay = 0.995 # Multiplicative decay factor for the exploration rate\n",
        "gamma = 0.9\n",
        "learning_rate=0.1\n",
        "\n",
        "agent = QL_agent(gridworld, gamma, learning_rate, eps_decay)\n",
        "\n",
        "# Iteration over all the episodes\n",
        "for episode in range(n_episodes):\n",
        "\n",
        "  state = gridworld.reset() # Setting the agent in the initial cell\n",
        "\n",
        "  for _ in range(n_steps_in_episode):\n",
        "    action = agent.get_action(state)  # get action\n",
        "    state_next, reward = gridworld.step(action)  # evolve state by action\n",
        "    agent.train((state, action, state_next, reward)) # train agent\n",
        "    state = state_next  # transition to next state\n",
        "\n",
        "  agent.epsilon = max(agent.epsilon * agent.epsilon_decay, 0.02) # Decrease the exploration\n",
        "  # Show training info\n",
        "  sys.stdout.write(\"\\rEpisode: \" + str(episode+1) + \"/\" + str(n_episodes) + \" epsilon: \" + str(agent.epsilon))\n",
        "  sys.stdout.flush()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1000/1000 epsilon: 0.02"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX48tyD45ihm",
        "outputId": "19094c04-033a-49e3-8eaa-f921e1219f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(world_size[0]/2.5 + 1, world_size[1]/2.5))\n",
        "gridworld.display_best_path(ax, agent.Q, start_cell, values=agent.values)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='x', ylabel='y'>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXUztOe-JQL_"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "- Try to change the exploration decay rate and see how a more \"exploitative\" or \"explorative\" protocols do. What do you notice? What are the pros and cons of the two?\n",
        "\n",
        "- Can you build a new map with one close small reward and one far away and large? Can you find two gammas such that for one value the agent prefers the small and for the other the large reward?\n",
        "Be careful of the learning parameters, if you complexify the system, you will probably need more episodes or explore more..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_SAOqmelo9A"
      },
      "source": [],
      "execution_count": 5,
      "outputs": []
    }
  ]
}